---
typora-copy-images-to: ../../public
typora-root-url: /Volumes/硬盘/Code/docs/docs/public
---

## Droplr



### 起因

Droplr最初是个LAMP 应用程序[^1]。后来随着发展，需要完全重新考虑 Droplr 的基础设施。

在那时, Droplr 本身已经确立成为了一种工作的理念, 因此 2.0 版本的目标也是相当的标准: 

*   将单片的技术栈拆分为多个可横向扩展的组件;
*   添加冗余,以避免宕机;
*   为客户端创建一个简洁的 API;
*   使其全部运行在 HTTPS 上。

### 工作原理

Droplr 拥有一个非常简单的工作流:将一个文件拖动到应用程序的菜单栏图标,然后 Droplr 将会上传该文件。当上传完成之后,Droplr 将复制一个短 URL——也就是所谓的拖乐(drop) —到剪贴板。

就是这样。欢畅地、实时地分享。 
而在幕后,拖乐元数据将会被存储到数据库中(包括创建日期、名称以及下载次数等信息) , 而文件本身则被存储在 Amazon S3 上。

### 创造一个更加快速的上传体验

Droplr 的第一个版本的上传流程是相当地天真可爱: 

(1)接收上传; 

(2)上传到 S3; 

(3)如果是图片,则创建略缩图; 

(4)应答客户端应用程序。

 更加仔细地看看这个流程, 你很快便会发现在第 2 步和第 3 步上有两个瓶颈。 不管从客户端上传到我们的服务器有多快,在实际的上传完成之后,直到成功地接收到响应之间,对于拖乐的创建总是会有恼人的间隔—因为对应的文件仍然需要被上传到 S3 中,并为其生成略缩图。

文件越大,间隔的时间也越长。对于非常大的文件来说,连接(客户端和服务器之间)最终将会在等待来自服务器的响应时超时。由于这个严重的问题,当时Droplr只可以提供单个文件最大 32MB的上传能力。 

有两种截然不同的方案来减少上传时间。 

*   方案 A,乐观且看似更加简单(见图 14-1) :
    *   完整地接收文件; 
    *   将文件保存到本地的文件系统,并立即返回成功到客户端;
    *   计划在将来的某个时间点将其上传到 S3。
*   方案 B,安全但复杂(见图 14-2) :
    *   实时地(流式地)将从客户端上传的数据直接管道给 S3。

<div class='flex flex-row'>
    <img src="/Netty实战_page_214_1.png" alt="图 14-1  方案 A,乐观且看似更加简单" style="zoom:25%;" />
    <img src="/Netty实战_page_214_2.png" alt="图 14-2  方案 B,安全但复杂" style="zoom:25%;" />
</div>


#### 乐观且看似更加简单的方案

在收到文件之后便返回一个短 URL 创造了一个空想 (也可以将其称为隐式的契约) , 即该文件立即在该 URL 地址上可用。但是并不能够保证,上传的第二阶段(实际将文件推送到 S3)也将最终会成功,那么用户可能会得到一个坏掉的链接,其可能已经被张贴到了 Twitter 或者发送给了一个重要的客户。这是不可接受的,即使是每十万次上传也只会发生一次。 

我们当前的数据显示,我们的上传失败率略低于 0.01%(万分之一) ,绝大多数都是在上传实际完成之前,客户端和服务器之间的连接就超时了。 

我们也可以尝试通过在文件被最终推送到 S3 之前,从接收它的机器提供该文件的服务来绕开它,然而这种做法本身就是一堆麻烦:

*   如果在一批文件被完整地上传到 S3 之前, 机器出现了故障, 那么这些文件将会永久丢失;
*   也将会有跨集群的同步问题( “这个拖乐所对应的文件在哪里呢?” ) ;
*   将会需要额外的复杂的逻辑来处理各种边界情况,继而不断产生更多的边界情况; 

在思考过每种变通方案和其陷阱之后, 我很快认识到, 这是一个经典的九头蛇问题——对于每个砍下的头,它的位置上都会再长出两个头。

#### 安全但复杂的方案

另一个选项需要对整体过程进行底层的控制。从本质上说,我们必须要能够做到以下几点。

*   在接收客户端上传文件的同时,打开一个到 S3 的连接。
*   将从客户端连接上收到的数据管道给到 S3 的连接。 
*   缓冲并节流这两个连接:
    *   需要进行缓冲,以在客户端到服务器,以及服务器到 S3 这两个分支之间保持一条的稳定的流;
    *   需要进行节流,以防止当服务器到 S3 的分支上的速度变得慢于客户端到服务器的分支时,内存被消耗殆尽。 
*   当出现错误时,需要能够在两端进行彻底的回滚。 

看起来概念上很简单,但是它并不是你的通常的 Web 服务器能够提供的能力。尤其是当你考虑节流一个 TCP 连接时,你需要对它的套接字进行底层的访问。 

它同时也引入了一个新的挑战,其将最终塑造我们的终极架构:推迟略缩图的创建。 

这也意味着,无论该平台最终构建于哪种技术栈之上,它都必须要不仅能够提供一些基本的特性,如难以置信的性能和稳定性,而且在必要时还要能够提供操作底层(即字节级别的控制)的灵活性。

### 技术栈

当开始一个新的 Web 服务器项目时,最终你将会问自己: “好吧,这些酷小子们这段时间都在用什么框架呢?”我也是这样的。 

选择 Netty 并不是一件无需动脑的事; 我研究了大量的框架, 并谨记我认为的 3 个至关重要的要素。  
	(1)它必须是快速的。我可不打算用一个低性能的技术栈替换另一个低性能的技术栈。 
	(2)它必须能够伸缩。不管它是有 1 个连接还是 10 000 个连接,每个服务器实例都必须要能够保持吞吐量,并且随着时间推移不能出现崩溃或者内存泄露。 
	(3)它必须提供对底层数据的控制。字节级别的读取、TCP 拥塞控制等,这些都是难点。 

要素 1 和要素 2 基本上排除了任何非编译型的语言。 我是 Ruby 语言的拥趸, 并且热爱 Sinatra 和 Padrino 这样的轻量级框架,但是我知道我所追寻的性能是不可能通过这些构件块实现的。 

要素 2 本身就意味着:无论是什么样的解决方案,它都不能依赖于阻塞 I/O。看到了本书这里,你肯定已经明白为什么非阻塞 I/O 是唯一的选择了。 

要素 3 比较绕弯儿。 它意味着必须要在一个框架中找到完美的平衡, 它必须在提供了对于它所接收到的数据的底层控制的同时,也支持快速的开发,并且值得信赖。这便是语言、文档、社区以及其他的成功案例开始起作用的时候了。 

在那时我有一种强烈的感觉:Netty 便是我的首选武器。

#### 基本要素:服务器和流水线

服务器基本上只是一个ServerBootstrap, 其内置了NioServerSocketChannelFactory, 配置了几个常见的 ChannelHandler 以及在末尾的 HTTP RequestController, 如代码清单14-1 所示。

```java [代码清单 14-1  设置 ChannelPipeline]
pipelineFactory = new ChannelPipelineFactory() {
    @Override public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        pipeline.addLast("idleStateHandler", new IdleStateHandler(...));
        pipeline.addLast("httpServerCodec", new HttpServerCodec());
        pipeline.addLast("requestController", new RequestController(...));
        return pipeline;
    }
};
```

RequestController 是 ChannelPipeline 中唯一自定义的 Droplr 代码,同时也可能是整个Web 服务器中最复杂的部分。它的作用是处理初始请求的验证,并且如果一切都没问题,那么将会把请求路由到适当的请求处理器。对于每个已经建立的客户端连接,都会创建一个新的实例, 并且只要连接保持活动就一直存在。 
请求控制器负责: 

*   处理负载洪峰; 
*   HTTP ChannelPipeline 的管理;
*   设置请求处理的上下文;
*   派生新的请求处理器; 
*   向请求处理器供给数据; 
*   处理内部和外部的错误。

代码清单 14-2 给出的是 RequestController 相关部分的一个纲要。

```java [代码清单 14-2  RequestController]
public class RequestController extends IdleStateAwareChannelUpstreamHandler {
    @Override
    public void channelIdle(ChannelHandlerContext ctx, IdleStateEvent e) throws Exception {
        // Shut down connection to client and roll everything back. 
    }
    @Override
    public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        if (!acquireConnectionSlot()) {
            // Maximum number of allowed server connections reached,             
            // respond with 503 service unavailable             
            // and shutdown connection. 
        } else {
            // Set up the connection's request pipeline. 
        }
    }
    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {
        if (isDone()) return;
        if (e.getMessage() instanceof HttpRequest) {
            handleHttpRequest((HttpRequest) e.getMessage()); // Droplr 的服务器请求验证的关键点
        } else if (e.getMessage() instanceof HttpChunk) {
            handleHttpChunk((HttpChunk) e.getMessage()); // 如果针对当前请求有一个活动的处理器, 并且它能够接受 HttpChunk 数据, 那么它将继续按HttpChunk 传递
        }
    }
}
```

如同本书之前所解释过的一样, 你应该永远不要在 Netty 的 I/O 线程上执行任何非 CPU 限定的代码——你将会从 Netty 偷取宝贵的资源,并因此影响到服务器的吞吐量。 

因此,HttpRequest 和 HttpChunk 都可以通过切换到另一个不同的线程,来将执行流程移交给请求处理器。当请求处理器不是 CPU 限定时,就会发生这样的情况,不管是因为它们访问了数据库,还是执行了不适合于本地内存或者 CPU 的逻辑。 

当发生线程切换时,所有的代码块都必须要以串行的方式执行;否则,我们就会冒风险,对于一次上传来说,在处理完了序列号为 n 的HttpChunk 之后,再处理序列号为 n - 1 的HttpChunk 必然会导致文件内容的损坏。 (我们可能会交错所上传的文件的字节布局。 )为了处理这种情况,我创建了一个自定义的线程池执行器, 其确保了所有共享了同一个通用标识符的任务都将以串行的方式被执行。  

从这里开始,这些数据(请求和 HttpChunk)便开始了在 Netty 和 Droplr 王国之外的冒险。  

我将简短地解释请求处理器是如何被构建的,以在 RequestController(其存在于 Netty 的领地)和这些处理器(存在于 Droplr 的领地)之间的桥梁上亮起一些光芒。谁知道呢,这也许将会帮助你架构你自己的服务器应用程序呢!

#### 请求处理器

#### 父接口

#### 处理器的实现

#### 上传请求处理器

### 性能



## Firebase

### 架构



### 长轮询



### KeepAlive和流水线优化



### 控制SslHanllder



## Urban AirShipAirShip

### 移动消息的基础知识



### 第三方递交



### 使用二进制协议的例子



### 直接面向设备的递交



### Netty擅长管理大量的并发连接



### Urban Airship小结



#### 内部RPC框架



#### 负载和性能测试



#### 同步协议的异步客户端





[^1]:一个典型的应用程序技术栈的首字母缩写;由 Linux、Apache Web Server、MySQL 以及 PHP 的首字母组成。
